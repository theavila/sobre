---
layout: tutorial
comments: true
title: Redução de Dimensionalidade com Algoritmo PCA
subtitle: "Análise de Componente Principal (PCA)"
lang: pt
date: 2020-07-16
true-dt: 2020-07-16
tags: [Tutorial,python,Jupyter]
author: "Ricardo Avila"
comments: true
header-img: "img/pca.png"
---
## Conteúdo

0. [Características do Algoritmo PCA](#modelo)
1. [Conjunto de Dados](#dados)
2. [Criando o Modelo de Treinamento](#treino)
3. [Considerações Finais](#fim)

## Características do Algoritmo Naïve Bayes <a name="modelo"></a>

O algoritmo de Análise de Componentes Principais (Principal Component Analysis, ou somente PCA) é uma modelo de aprendizagem de máquina não supervisionado utilizado para tentar reduzir a dimensionalidade (número de recursos) de um conjunto de dados, tentando, ainda, manter o maior número possível de informações. É um dos métodos multivariado mais utilizados e conhecido para a redução de dados.

O algoritmo usa um conjunto de recursos representado por uma matriz de <strong>N</strong> registros por <strong>P</strong> atributos, que podem estar correlacionados entre si, e sumariza esse conjunto por eixos não correlacionados (componentes principais) que são uma combinação linear das <strong>P</strong> variáveis originais. O primeiro componente represente a maior variabilidade possível nos dados, o segundo componente, a segunda maior variabilidade, e assim por diante. 

O algoritmo PCA usa a distância euclidiana calculada a partir dos <strong>P</strong> atributos como uma medida de dissimilaridade entre os <strong>N</strong> objetos. Para isso, o algoritmo PCA calcula as <strong>K</strong> melhores possíveis dimensões (<strong>K</strong> < <strong>P</strong>) representandos a distância euclidiana entre os objetos.

Para esse exemplo, primeiramente iremos importar as bibliotecas básicas que serão utilizadas nesse exemplo.

{% highlight python %}
from numpy import *
from pylab import *
{% endhighlight %}

Além da biblioteca NumPy, também iremos utilizar a biblioteca PyLab que permite gerar gráficos de duas dimensões de excelente qualidade, permitindo edição interativa, animações, inúmeros tipos de gráficos diferentes e o salvamento das imagens geradas em diversos formatos diferentes.

## Conjunto de Dados <a name="dados"></a>

Para esse exemplo, mais uma vez foi utilizado o conjunto de dados Iris. Esse conjunto possui 150 registros de 3 espécies diferentes de flor Iris: Versicolor, Setosa e Virginica. Cada registro do conjunto possui cinco características: SepalLength (Comprimento da Sépala), SepalWidth (Largura da Sépala), PetalLength (Comprimento da Pétala), PetalWidth (Largura da Pétala) e class (Classe).

Dataset disponível em: <a href="https://archive.ics.uci.edu/ml/datasets/Iris" target="_blank">https://archive.ics.uci.edu/ml/datasets/Iris</a>

{% highlight python %}
f = open("iris.csv","r")
setosa = []
versicolor = []
virginicia = []
{% endhighlight %}

Já aproveitamos para criar os dataframes com as classes de flores iris presentes no conjunto de dados. Desse modo, já deixamos tudo preparado para os próximos passos.

{% highlight python %}
for data in f.readlines():
    val = data.split(",")
    iris_type = val[-1].rsplit()[0]
    values = [double(i) for i in val[:-1]]    
    if(iris_type == "Iris-setosa"):
        setosa.append(values) 
    if(iris_type == "Iris-versicolor"):
        versicolor.append(values)
    if(iris_type == "Iris-virginicia"):
        virginicia.append(values)
{% endhighlight %}

## Criando o Modelo de Treinamento <a name="treino"></a>

Dessa vez optamos por criar o códido do algoritmo PCA do zero, sem utilizar nenhuma biblioteca. Para isso, definimos a função a seguir:

{% highlight python %}
def pca(X,reduced_dimension=None):

    samples,dim = X.shape    
    
    X = (X-X.mean(axis = 0))/(X.var(axis=0))**(1/2.0)  
    
    U,S,V = linalg.svd(X)       

    if reduced_dimension:
        V = V[:reduced_dimension]
    return V.T
{% endhighlight %}

No algoritmo PCA, os objetos são representados por uma nuvem de <strong>N</strong> pontos em um espaço multidimensional, com um eixo para cada uma dos <strong>P</strong> atributos, de modo que:
* o centroide dos pontos é definido pela média de cada atributo; e
* a variância de cada atributo é média dos quadrados da diferença dos <strong>N</strong> pontos com relação a média de cada atributo.

Desse modo, a fórmula geométrica do PCA é:

<img class="img-responsive center-block thumbnail" src="/img/pcaFormula.png" alt="pca-Formula" style="width:75%"/>

O passo seguinte é computar a matriz de projeção dos componentes principais utilizando a função PCA para cada uma das classe. O número 2 que foi atribuído significa que queremos que nossa matriz de projeção seja projetada em duas dimensões.

{% highlight python %}
setosa_pc_projection_matrix = pca(array(setosa),2)
versicolor_pc_projection_matrix =  pca(array(versicolor),2)
virginicia_pc_projection_matrix =  pca(array(virginicia),2) 

low_dim_setosa_points = dot(array(setosa),setosa_pc_projection_matrix)
low_dim_versicolor_points = dot(array(versicolor),versicolor_pc_projection_matrix)
low_dim_virginicia_points = dot(array(virginicia),virginicia_pc_projection_matrix)
{% endhighlight %}

Pronto! Os componentes principais foram projetados. Agora vamos plotar os dados para verificar o resultado.

{% highlight python %}
p1 = plot(low_dim_setosa_points[:,0].tolist(),low_dim_setosa_points[:,1].tolist(),"ro", label="Iris Setosa", color='green')
p2 = plot(low_dim_versicolor_points[:,0].tolist(),low_dim_versicolor_points[:,1].tolist(),"r*",label="Iris versicolor", color='red')
p3 = plot(virginicia_pc_projection_matrix[:,0].tolist(),virginicia_pc_projection_matrix[:,1].tolist(),"r^",label="Iris virginicia", color='blue')

title("Redução de Dimensionalidade com PCA")
xlabel("X1")
ylabel("X2") 
legend(loc = "lower right")
show()
{% endhighlight %}

<img class="img-responsive center-block thumbnail" src="/img/pcaPlot.png" alt="pca-Plot" style="width:75%"/>

## Considerações Finais <a name="fim"></a>

Na prática, os algoritmo PCA não é utilizado em conjunto de dados com pouicas variáveis. Precisa ter, pelo menos 3 ou mais dimensões para que ele possa ser aplicado. Outro ponto importante sobre o algoritmo PCA é que ele funciona melhor quando as variáveis estão representadas na mesma unidade. Caso contrário, as variáveis com alta variância irão dominar os componentes principais. Para evitar esse problema, recomenda-se normalizar os atributos.

Fica mais uma vez a dica de aplicar esse modelo em outras bases disponibilizadas na Internet, bastando fazer alguns ajustes caso seja necessário. Todo o código e mais um pouco está disponível no meu <a href="https://github.com/theavila/tutoriaisML">GitHub</a>.

Os passos de execução deste tutorial foram testados com `Python 3.6` e tudo ocorreu sem problemas. No entanto, é possível que alguém encontre alguma dificuldade ou erro no meio do caminho. Se for o caso, por favor comente a sua dificuldade ou erro neste post.